import { test, expect } from "@jest/globals";
import { Document } from "../../document.js";
import { BaseLLM } from "../../llms/index.js";
import { loadQAMapReduceChain } from "../question_answering/load.js";
class FakeLLM extends BaseLLM {
    constructor() {
        super(...arguments);
        Object.defineProperty(this, "nrMapCalls", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 0
        });
        Object.defineProperty(this, "nrReduceCalls", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 0
        });
    }
    _llmType() {
        return "fake";
    }
    async _generate(prompts, _) {
        return {
            generations: prompts.map((prompt) => {
                let completion = "";
                if (prompt.startsWith("Use the following portion")) {
                    this.nrMapCalls += 1;
                    completion = "a portion of context";
                }
                else if (prompt.startsWith("Given the following extracted")) {
                    this.nrReduceCalls += 1;
                    completion = "a final answer";
                }
                return [
                    {
                        text: completion,
                        score: 0,
                    },
                ];
            }),
        };
    }
}
test("Test MapReduceDocumentsChain", async () => {
    const model = new FakeLLM({});
    const chain = loadQAMapReduceChain(model);
    const docs = [
        new Document({ pageContent: "harrison went to harvard" }),
        new Document({ pageContent: "ankush went to princeton" }),
    ];
    const res = await chain.call({
        input_documents: docs,
        question: "Where did harrison go to college",
    });
    console.log({ res });
    expect(res).toEqual({
        text: "a final answer",
    });
    expect(model.nrMapCalls).toBe(0); // below maxTokens
    expect(model.nrReduceCalls).toBe(1);
});
test("Test MapReduceDocumentsChain with content above maxTokens", async () => {
    const model = new FakeLLM({});
    const chain = loadQAMapReduceChain(model);
    const aString = "a".repeat(10000);
    const bString = "b".repeat(10000);
    const docs = [
        new Document({ pageContent: aString }),
        new Document({ pageContent: bString }),
    ];
    const res = await chain.call({
        input_documents: docs,
        question: "Is the letter c present in the document",
    });
    console.log({ res });
    expect(res).toEqual({
        text: "a final answer",
    });
    expect(model.nrMapCalls).toBe(2); // above maxTokens
    expect(model.nrReduceCalls).toBe(1);
});
//# sourceMappingURL=combine_docs_chain.test.js.map